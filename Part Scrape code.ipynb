{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a880d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dependencies\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b1cca",
   "metadata": {},
   "source": [
    "### What's currently missing code-wise:\n",
    "\n",
    "Removed proxy implementation code for the moment\n",
    "Sub-cat and part code does not have implementation for checking already scraped code (should be able to just compare url's present in sub-cat or parts to see if that part has been scraped or not)\n",
    "Search feature to only scrape parts by specific priority manufacturers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manufacturer scraping code(complete)\n",
    "\n",
    "url = \"https://www.rockauto.com/en/catalog/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser').find_all('div', attrs={'class', 'ranavnode'})\n",
    "soup\n",
    "soup_filter = []\n",
    "makes_list = []\n",
    "# Find US Market Only\n",
    "for x in soup:\n",
    "    soup_filter.append( x.find('a', attrs={'class', 'navlabellink'}) )\n",
    "\n",
    "# Get [Make, Year, Model, Link]\n",
    "for x in soup_filter:\n",
    "    makes_list.append( {'make': x.get_text(), 'link': 'https://www.rockauto.com' + str( x.get('href') ) })\n",
    "makes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a85b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makelist by Manufacturer (complete)\n",
    "\n",
    "makes_list = makes_list[:]\n",
    "makes = []\n",
    "for car in makes_list:\n",
    "    search_make = car[\"make\"]\n",
    "    url = car[\"link\"]\n",
    "    \n",
    "    years_list = []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser').find_all('div', attrs={'class': 'ranavnode'})\n",
    "    soup = soup[1:]\n",
    "    soup_filter = []\n",
    "\n",
    "    # Find US Market Only\n",
    "    for x in soup:\n",
    "        if 'US' in next(x.children)['value']:\n",
    "            soup_filter.append( x.find('a', attrs={'class': 'navlabellink'}) )\n",
    "    # Get [Make, Year, Model, Link]\n",
    "    for x in soup_filter:\n",
    "        makes.append({'make': search_make, 'year': x.get_text(), 'link': 'https://www.rockauto.com' + str( x.get('href') ) })\n",
    "    \n",
    "makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to vehicles after 2015\n",
    "makes = makes[makes['year'] > 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d22ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Vehicle model list (complete)\n",
    "\n",
    "models = []\n",
    "output_file = \"scraped_models.json\"\n",
    "\n",
    "for index, (search_make, url, search_year) in enumerate(zip(makes[\"make\"], makes[\"link\"], makes[\"year\"]), 1):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    time.sleep(random.uniform(1, 3))  # Introduce a random delay between 1 and 3 seconds\n",
    "    soup = BeautifulSoup(response.text, 'html.parser').find_all('div', attrs={'class': 'ranavnode'})\n",
    "    soup = soup[1:]\n",
    "    soup_filter = []\n",
    "\n",
    "    for x in soup:\n",
    "        if 'US' in next(x.children)['value']:\n",
    "            soup_filter.append(x.find('a', attrs={'class', 'navlabellink'}))\n",
    "\n",
    "    # Get [Make, Year, Model, Link]\n",
    "    for x in soup_filter:\n",
    "        model = {'make': search_make, 'year': search_year, 'model': x.get_text(), 'link': 'https://www.rockauto.com' + str(x.get('href'))}\n",
    "        models.append(model)\n",
    "        with open(output_file, 'a') as f:\n",
    "            json.dump(model, f, indent=4)\n",
    "            f.write('\\n')\n",
    "        \n",
    "        time.sleep(random.uniform(1, 3))  # Introduce a random delay between 1 and 3 seconds before next request\n",
    "\n",
    "# Print response after all iterations are done\n",
    "print(\"Response:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32118524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engine scraping code (complete)\n",
    "models = models[:]\n",
    "output_file = \"scraped_engines.json\"\n",
    "\n",
    "for car in models:\n",
    "    search_make = car[\"make\"]\n",
    "    url = car[\"link\"]\n",
    "    search_year = car[\"year\"]\n",
    "    search_model = car[\"model\"]\n",
    "\n",
    "    # Check if this car already exists in engines\n",
    "    if any(engine['make'] == search_make and engine['year'] == search_year and engine['model'] == search_model for engine in engines):\n",
    "        continue  # Skip this car if it already exists\n",
    "\n",
    "    response = requests.get(url)\n",
    "    time.sleep(random.uniform(1, 3))  # Introduce a random delay between 1 and 3 seconds\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser').find_all('div', attrs={'class': 'ranavnode'})\n",
    "    soup = soup[1:]\n",
    "    soup_filter = []\n",
    "\n",
    "    for x in soup:\n",
    "        if 'US' in next(x.children)['value']:\n",
    "            soup_filter.append( x.find('a', attrs={'class', 'navlabellink'}) )\n",
    "\n",
    "    # Get [Make, Year, Model, Link]\n",
    "    for x in soup_filter:\n",
    "        \n",
    "        engine = {'make': search_make, 'year': search_year, 'model': search_model, 'engine': x.get_text(), 'link': 'https://www.rockauto.com' + str( x.get('href') ) }\n",
    "        engines.append(engine)\n",
    "        with open(output_file, 'a') as f:\n",
    "            json.dump(engine, f, indent=4)\n",
    "            f.write('\\n')\n",
    "        \n",
    "        time.sleep(random.uniform(1, 3))  # Introduce a random delay between 1 and 3 seconds before next request\n",
    "    \n",
    "engines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories scrape code (with rotating user agents, vastly incomplete as well due to lack of proxies)\n",
    "from fake_useragent import UserAgent\n",
    "import random \n",
    "engines = engines[0:1]\n",
    "categories = []\n",
    "e = engines[:]\n",
    "output_file = \"categories.json\"\n",
    "ua = UserAgent()\n",
    "\n",
    "for car in e:\n",
    "\n",
    "    search_make = car[\"make\"]\n",
    "    url = car[\"link\"]\n",
    "    search_year = car[\"year\"]\n",
    "    search_model = car[\"model\"]\n",
    "    search_engine = car[\"engine\"]\n",
    "    headers = {'User-Agent': ua.random}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    if any(engine['make'] == search_make and engine['year'] == search_year and engine['model'] == search_model and engine['engine'] == search_engine for engine in categories):\n",
    "        continue  # Skip this car if it already exists\n",
    "  \n",
    "    soup = BeautifulSoup(response.text, 'html.parser').find_all('a', attrs={'class', 'navlabellink'})[4:]\n",
    "    for x in soup:\n",
    "        response = requests.get('https://www.rockauto.com' + str( x.get('href') ))\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser').find_all('a', attrs={'class', 'navlabellink'})[5:]\n",
    "        categories.append({'make': search_make, 'year': search_year, 'model': search_model, 'engine': search_engine, 'category': x.get_text()})\n",
    "        print(categories)\n",
    "        with open(output_file, 'a') as f:\n",
    "                        json.dump(categories[-1], f, indent=4)\n",
    "                        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sub-cat scrape code (incomplete, categories isn't done scraping)\n",
    "categories = categories[0:2]\n",
    "subcat = []\n",
    "for car in categories:\n",
    "    search_make = car[\"make\"]\n",
    "    url = car[\"link\"]\n",
    "    search_year = car[\"year\"]\n",
    "    search_model = car[\"model\"]\n",
    "    search_engine = car[\"engine\"]\n",
    "    search_category = car[\"category\"]\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser').find_all('a', attrs={'class', 'navlabellink'})[5:]\n",
    "\n",
    "    for x in soup:\n",
    "        subcat.append( {'make': search_make, 'year': search_year, 'model': search_model, 'engine': search_engine, 'category': search_category, 'sub_category': x.get_text(), 'link': 'https://www.rockauto.com' + str( x.get('href') ) })\n",
    "        \n",
    "    \n",
    "subcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part_list scrape code (incomplete, categories and subcategories aren't done scraping)\n",
    "\n",
    "part_list = []\n",
    "for car in subcat:\n",
    "    search_make = car[\"make\"]\n",
    "    url = car[\"link\"]\n",
    "    search_year = car[\"year\"]\n",
    "    search_model = car[\"model\"]\n",
    "    search_engine = car[\"engine\"]\n",
    "    search_category = car[\"category\"]\n",
    "    search_subcat = car[\"sub_category\"]\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    for x in soup:    \n",
    "        #Parts\n",
    "        for product_block in soup.find_all('div', class_='listing-text-row-moreinfo-truck'):  \n",
    "            # Create a dictionary for this product\n",
    "            product_dict = {\n",
    "                'make': search_make, 'year': search_year, 'model': search_model, 'engine': search_engine, 'category': search_category, 'sub_category': search_subcat,\n",
    "                'manufacturer': product_block.find('span', class_='listing-final-manufacturer').text,\n",
    "                'part_number': product_block.find('span', class_='listing-final-partnumber').text\n",
    "                #'description': product_block.find('span', class_='span-link-underline-remover').text,\n",
    "            }\n",
    "            try:\n",
    "                product_dict['link'] = product_block.find('a', class_='ra-btn ra-btn-moreinfo')['href']\n",
    "            except TypeError:\n",
    "                product_dict['link'] = None\n",
    "\n",
    "            \n",
    "            part_list.append(product_dict)\n",
    "        \n",
    "    \n",
    "part_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf62757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search make Query\n",
    "search_query = {'make': \"FORD\", 'model': \"ESCAPE\", 'year': \"2023\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to scrape part_list (works completely, however unable to run without proxies)\n",
    "#Has to be run in one shot, unable to do partial scrapes and pick up on where the last iteration left off\n",
    "engines = engines[:]\n",
    "categories = []\n",
    "part_list = []\n",
    "output_file = \"part_list.json\"\n",
    "\n",
    "# Initialize an empty filter\n",
    "filt = {}\n",
    "\n",
    "# Add values to the filter if they exist in search_query\n",
    "for key, value in search_query.items():\n",
    "    if value: \n",
    "        filt[key] = value\n",
    "for car in engines:\n",
    "    search_make = car[\"make\"]\n",
    "    url = car[\"link\"]\n",
    "    search_year = car[\"year\"]\n",
    "    search_model = car[\"model\"]\n",
    "    search_engine = car[\"engine\"]\n",
    "    c = {\n",
    "    \"make\": search_make,\n",
    "    \"link\": url,\n",
    "    \"year\": search_year,\n",
    "    \"model\": search_model,\n",
    "    \"engine\": search_engine\n",
    "    }\n",
    "    all_values_match = True\n",
    "    for key in filt:\n",
    "        if key in c and filt[key] != c[key]:\n",
    "            all_values_match = False\n",
    "            break\n",
    "\n",
    "    if all_values_match:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser').find_all('a', attrs={'class', 'navlabellink'})[4:]\n",
    "        time.sleep(random.uniform(2, 3))\n",
    "        for x in soup:\n",
    "            #categories\n",
    "            response = requests.get('https://www.rockauto.com' + str( x.get('href') ))\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser').find_all('a', attrs={'class', 'navlabellink'})[5:]\n",
    "            time.sleep(random.uniform(2, 3))\n",
    "            for y in soup:\n",
    "                #Sub-cat\n",
    "                response = requests.get('https://www.rockauto.com' + str( y.get('href') ))\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                time.sleep(random.uniform(2, 3))\n",
    "                for j in soup:\n",
    "                    #Parts\n",
    "                    for product_block in soup.find_all('div', class_='listing-text-row-moreinfo-truck'):  \n",
    "                        # Create a dictionary for this product\n",
    "                        product_dict = {\n",
    "                            'make': search_make, 'year': search_year, 'model': search_model, 'engine': search_engine, 'category': x.get_text(), 'sub_category': y.get_text(),\n",
    "                            'manufacturer': product_block.find('span', class_='listing-final-manufacturer').text,\n",
    "                            'part_number': product_block.find('span', class_='listing-final-partnumber').text\n",
    "                            #'description': product_block.find('span', class_='span-link-underline-remover').text,\n",
    "                        }\n",
    "                        try:\n",
    "                            product_dict['link'] = product_block.find('a', class_='ra-btn ra-btn-moreinfo')['href']\n",
    "                        except TypeError:\n",
    "                            product_dict['link'] = None\n",
    "\n",
    "                        with open(output_file, 'a') as f:\n",
    "                            json.dump(product_dict, f, indent=4)\n",
    "                            f.write('\\n')\n",
    "                        part_list.append(product_dict)\n",
    "                        time.sleep(random.uniform(2, 3))\n",
    "part_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load scraped models (complete)\n",
    "data_list = []\n",
    "\n",
    "with open('scraped_models.txt', 'r') as file:\n",
    "    data = file.read().split('\\n}\\n{')\n",
    "    for item in data:\n",
    "        item = item.replace('{', '').replace('}', '').split(',\\n')\n",
    "        dict_data = {}\n",
    "        for element in item:\n",
    "            key, value = element.strip().split(': ')\n",
    "            dict_data[key.strip('\"')] = value.strip('\"')\n",
    "        data_list.append(dict_data)\n",
    "\n",
    "data_list\n",
    "models = data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load current scraped engines (complete)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open('scraped_engines.json', 'r') as file:\n",
    "    data = file.read().split('\\n}\\n{')\n",
    "    for item in data:\n",
    "        item = item.replace('{', '').replace('}', '').split(',\\n')\n",
    "        dict_data = {}\n",
    "        for element in item:\n",
    "            key, value = element.strip().split(': ')\n",
    "            dict_data[key.strip('\"')] = value.strip('\"')\n",
    "        data_list.append(dict_data)\n",
    "\n",
    "data_list\n",
    "engines = data_list\n",
    "engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load current scraped categories (don't load this, scrape from scratch)(lacks URL link, so start scraping from scratch)\n",
    "data_list = []\n",
    "\n",
    "with open('categories.json', 'r') as file:\n",
    "    data = file.read().split('\\n}\\n{')\n",
    "    for item in data:\n",
    "        item = item.replace('{', '').replace('}', '').split(',\\n')\n",
    "        dict_data = {}\n",
    "        for element in item:\n",
    "            key, value = element.strip().split(': ')\n",
    "            dict_data[key.strip('\"')] = value.strip('\"')\n",
    "        data_list.append(dict_data)\n",
    "\n",
    "data_list\n",
    "categories = data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load current scraped part_list (very incomplete, just here to look at what the final product looks like)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open('part_list.json', 'r') as file:\n",
    "    data = file.read().split('\\n}\\n{')\n",
    "    for item in data:\n",
    "        item = item.replace('{', '').replace('}', '').split(',\\n')\n",
    "        dict_data = {}\n",
    "        for element in item:\n",
    "            key, value = element.strip().split(': ')\n",
    "            dict_data[key.strip('\"')] = value.strip('\"')\n",
    "        data_list.append(dict_data)\n",
    "\n",
    "data_list\n",
    "part_list = data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df0cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0a452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
